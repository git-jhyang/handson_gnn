{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gzip, pickle, time, torch, os, sys\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import numpy as np\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pytorch를 활용한 Network 정의"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 간단한 Fully-connected neural network 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FCNN(nn.Module):\n",
    "    # 모델 정의\n",
    "    def __init__(self, \n",
    "                 num_feat,\n",
    "                 num_output,\n",
    "                 num_node = 32,\n",
    "                 num_layer = 4,\n",
    "                 dropout_rate = 0,\n",
    "                 batch_norm = False,\n",
    "        ):\n",
    "        # nn.Module 초기화\n",
    "        super(FCNN, self).__init__()\n",
    "        \n",
    "        # embedding layer 정의\n",
    "        self.embed = nn.Sequential(\n",
    "            nn.Linear(num_feat, num_node), # Linear layer\n",
    "            nn.ReLU(), # activation function\n",
    "        )\n",
    "        \n",
    "        # hidden layer 정의\n",
    "        self.hidden = nn.ModuleList()\n",
    "        for _ in range(num_layer):\n",
    "            hidden_layer = [] # List 형태로 만든 후 순차적으로 layer 요소 추가\n",
    "            hidden_layer.append(nn.Linear(num_node, num_node)) \n",
    "            hidden_layer.append(nn.ReLU())\n",
    "\n",
    "            # Layer normalization, gradient가 발산/수렴하는 것을 방지해서 훈련 효율을 높임\n",
    "            # 둘 중 하나만 사용하면 됨\n",
    "            if dropout_rate != 0: # Dropout 정의\n",
    "                hidden_layer.append(nn.Dropout(dropout_rate))\n",
    "            if batch_norm: # batch normalization 정의\n",
    "                hidden_layer.append(nn.BatchNorm1d(num_node))\n",
    "            self.hidden.append(nn.Sequential(*hidden_layer))\n",
    "\n",
    "        self.output = nn.Sequential(\n",
    "            nn.Linear(num_node, num_output),\n",
    "        )\n",
    "    \n",
    "    # 순방향 함수 정의\n",
    "    # input x가 들어와서 어떤 연산을 거쳐 output이 될 지 정의하는 함수\n",
    "    def forward(self, x):\n",
    "        # embedding layer 통과\n",
    "        h = self.embed(x)\n",
    "        # 각 layer를 순차적으로 통과\n",
    "        for layer in self.hidden:\n",
    "            h = layer(h)\n",
    "        # output layer 통과\n",
    "        out = self.output(h)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convolution neural network 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN2D(nn.Module):\n",
    "    # 모델 정의\n",
    "    def __init__(self, \n",
    "                 in_channel,\n",
    "                 num_output,\n",
    "                 out_channel = 32,\n",
    "                 kernel_size = 4,\n",
    "                 stride = 1,\n",
    "                 padding = 0,\n",
    "                 dilation = 1,\n",
    "                 num_layer = 3,\n",
    "                 dropout_rate = 0,\n",
    "                 batch_norm = False,\n",
    "        ):\n",
    "        # nn.Module 초기화\n",
    "        super(CNN2D, self).__init__()\n",
    "        self.embed = nn.Sequential(\n",
    "            nn.Conv2d(in_channel, out_channel, kernel_size, stride, padding, dilation),\n",
    "            nn.LeakyReLU(0.1)\n",
    "        )\n",
    "        # hidden layer 정의\n",
    "        self.hidden = nn.ModuleList()\n",
    "        for _ in range(num_layer):\n",
    "            hidden_layer = [] # List 형태로 만든 후 순차적으로 layer 요소 추가\n",
    "            hidden_layer.append(nn.Conv2d(out_channel, out_channel, kernel_size, stride, padding, dilation)) \n",
    "            hidden_layer.append(nn.LeakyReLU(0.1))\n",
    "\n",
    "            # Layer normalization, gradient가 발산/수렴하는 것을 방지해서 훈련 효율을 높임\n",
    "            # 둘 중 하나만 사용하면 됨\n",
    "            if dropout_rate != 0: # Dropout 정의\n",
    "                hidden_layer.append(nn.Dropout(dropout_rate))\n",
    "            if batch_norm: # batch normalization 정의\n",
    "                hidden_layer.append(nn.BatchNorm2d(out_channel))\n",
    "            hidden_layer.append(nn.MaxPool2d(2))\n",
    "            self.hidden.append(nn.Sequential(*hidden_layer))\n",
    "\n",
    "        self.output = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(512, num_output)\n",
    "        )\n",
    "    \n",
    "    # 순방향 함수 정의\n",
    "    # input x가 들어와서 어떤 연산을 거쳐 output이 될 지 정의하는 함수\n",
    "    def forward(self, x):\n",
    "        # embedding layer 통과\n",
    "        h = self.embed(x)\n",
    "#        print(h.shape)\n",
    "        # 각 layer를 순차적으로 통과\n",
    "        for layer in self.hidden:\n",
    "            h = layer(h)\n",
    "#            print(h.shape)\n",
    "        # output layer 통과\n",
    "        out = self.output(h)\n",
    "#        print(out.shape)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MNIST dataset 테스트"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
       "        [0, 0, 0,  ..., 0, 0, 0],\n",
       "        [0, 0, 0,  ..., 0, 0, 0],\n",
       "        ...,\n",
       "        [0, 0, 0,  ..., 0, 0, 0],\n",
       "        [0, 0, 0,  ..., 0, 0, 0],\n",
       "        [0, 0, 0,  ..., 0, 0, 0]], device='cuda:0', dtype=torch.int16)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with gzip.open('../data/mnist.pkl.gzip','rb') as f:\n",
    "    mnist = pickle.load(f)\n",
    "\n",
    "x_mnist_train = mnist['train']['x']\n",
    "y_mnist_train = mnist['train']['y']\n",
    "x_mnist_test = mnist['test']['x']\n",
    "y_mnist_test = mnist['test']['y']\n",
    "\n",
    "x_train = torch.from_numpy(x_mnist_train).short() # 16-bit integer\n",
    "x_train.to(device)\n",
    "# long code ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FlatData(Dataset):\n",
    "    def __init__(self, x, y, dtype=torch.float, device='cpu'):\n",
    "        # Dataset 초기화\n",
    "        super(FlatData, self).__init__()\n",
    "        # Input이 Tensor일 경우와 Tensor가 아닐 경우\n",
    "        if isinstance(x, torch.Tensor):\n",
    "            self._x = x.type(dtype).to(device)\n",
    "        else:\n",
    "            self._x = torch.tensor(x, dtype=dtype).to(device)\n",
    "        \n",
    "        # Label에 대해서도 동일한 작업 수행\n",
    "        if isinstance(y, torch.Tensor):\n",
    "            self._y = y.type(dtype).to(device)\n",
    "        else:\n",
    "            self._y = torch.tensor(y, dtype=dtype).to(device)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self._x.shape[0]\n",
    "    \n",
    "    def __getitem__(self, i):\n",
    "        return self._x[i], self._y[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "           0,  13,  25, 100, 122,   7,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  33,\n",
       "         151, 208, 252, 252, 252, 146,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  40, 152, 244,\n",
       "         252, 253, 224, 211, 252, 232,  40,   0,   0,   0,   0,   0,   0,   0,\n",
       "           0,   0,   0,   0,   0,   0,   0,   0,   0,  15, 152, 239, 252, 252,\n",
       "         252, 216,  31,  37, 252, 252,  60,   0,   0,   0,   0,   0,   0,   0,\n",
       "           0,   0,   0,   0,   0,   0,   0,   0,   0,  96, 252, 252, 252, 252,\n",
       "         217,  29,   0,  37, 252, 252,  60,   0,   0,   0,   0,   0,   0,   0,\n",
       "           0,   0,   0,   0,   0,   0,   0,   0,   0, 181, 252, 252, 220, 167,\n",
       "          30,   0,   0,  77, 252, 252,  60,   0,   0,   0,   0,   0,   0,   0,\n",
       "           0,   0,   0,   0,   0,   0,   0,   0,   0,  26, 128,  58,  22,   0,\n",
       "           0,   0,   0, 100, 252, 252,  60,   0,   0,   0,   0,   0,   0,   0,\n",
       "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "           0,   0,   0, 157, 252, 252,  60,   0,   0,   0,   0,   0,   0,   0,\n",
       "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0, 110,\n",
       "         121, 122, 121, 202, 252, 194,   3,   0,   0,   0,   0,   0,   0,   0,\n",
       "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  10,  53, 179, 253,\n",
       "         253, 255, 253, 253, 228,  35,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "           0,   0,   0,   0,   0,   0,   0,   0,   5,  54, 227, 252, 243, 228,\n",
       "         170, 242, 252, 252, 231, 117,   6,   0,   0,   0,   0,   0,   0,   0,\n",
       "           0,   0,   0,   0,   0,   0,   0,   6,  78, 252, 252, 125,  59,   0,\n",
       "          18, 208, 252, 252, 252, 252,  87,   7,   0,   0,   0,   0,   0,   0,\n",
       "           0,   0,   0,   0,   0,   0,   5, 135, 252, 252, 180,  16,   0,  21,\n",
       "         203, 253, 247, 129, 173, 252, 252, 184,  66,  49,  49,   0,   0,   0,\n",
       "           0,   0,   0,   0,   0,   3, 136, 252, 241, 106,  17,   0,  53, 200,\n",
       "         252, 216,  65,   0,  14,  72, 163, 241, 252, 252, 223,   0,   0,   0,\n",
       "           0,   0,   0,   0,   0, 105, 252, 242,  88,  18,  73, 170, 244, 252,\n",
       "         126,  29,   0,   0,   0,   0,   0,  89, 180, 180,  37,   0,   0,   0,\n",
       "           0,   0,   0,   0,   0, 231, 252, 245, 205, 216, 252, 252, 252, 124,\n",
       "           3,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "           0,   0,   0,   0,   0, 207, 252, 252, 252, 252, 178, 116,  36,   4,\n",
       "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "           0,   0,   0,   0,   0,  13,  93, 143, 121,  23,   6,   0,   0,   0,\n",
       "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
       "        dtype=torch.int16),\n",
       " tensor(2, dtype=torch.int16))"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset = FlatData(x_mnist_train, y_mnist_train, dtype=torch.short)\n",
    "test_dataset = FlatData(x_mnist_test, y_mnist_test, dtype=torch.short)\n",
    "\n",
    "DataLoader(train_dataset)\n",
    "DataLoader(test_dataset)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gnn",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
