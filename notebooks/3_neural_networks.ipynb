{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gzip, pickle, time, torch, os, sys\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pytorch를 활용한 Network 정의"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 간단한 Fully-connected neural network 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FCNN(nn.Module):\n",
    "    # 모델 정의\n",
    "    def __init__(self, \n",
    "                 num_feat,\n",
    "                 num_output,\n",
    "                 num_node = 32,\n",
    "                 num_layer = 4,\n",
    "                 dropout_rate = 0,\n",
    "                 batch_norm = False,\n",
    "        ):\n",
    "        # nn.Module 초기화\n",
    "        super(FCNN, self).__init__()\n",
    "        \n",
    "        # embedding layer 정의\n",
    "        self.embed = nn.Sequential(\n",
    "            nn.Linear(num_feat, num_node), # Linear layer\n",
    "            nn.ReLU(), # activation function\n",
    "        )\n",
    "        \n",
    "        # hidden layer 정의\n",
    "        self.hidden = nn.ModuleList()\n",
    "        for _ in range(num_layer):\n",
    "            hidden_layer = [] # List 형태로 만든 후 순차적으로 layer 요소 추가\n",
    "            hidden_layer.append(nn.Linear(num_node, num_node)) \n",
    "            hidden_layer.append(nn.ReLU())\n",
    "\n",
    "            # Layer normalization, gradient가 발산/수렴하는 것을 방지해서 훈련 효율을 높임\n",
    "            # 둘 중 하나만 사용하면 됨\n",
    "            if dropout_rate != 0: # Dropout 정의\n",
    "                hidden_layer.append(nn.Dropout(dropout_rate))\n",
    "            if batch_norm: # batch normalization 정의\n",
    "                hidden_layer.append(nn.BatchNorm1d(num_node))\n",
    "            self.hidden.append(nn.Sequential(*hidden_layer))\n",
    "\n",
    "        self.output = nn.Linear(num_node, num_output)\n",
    "    \n",
    "    # 순방향 함수 정의\n",
    "    # input x가 들어와서 어떤 연산을 거쳐 output이 될 지 정의하는 함수\n",
    "    def forward(self, x):\n",
    "        # embedding layer 통과\n",
    "        h = self.embed(x)\n",
    "        # 각 layer를 순차적으로 통과\n",
    "        for layer in self.hidden:\n",
    "            h = layer(h)\n",
    "        # output layer 통과\n",
    "        out = self.output(h)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Convolution neural network 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN2D(nn.Module):\n",
    "    # 모델 정의\n",
    "    def __init__(self, \n",
    "                 in_channel,\n",
    "                 num_output,\n",
    "                 out_channel = 32,\n",
    "                 kernel_size = 4,\n",
    "                 stride = 1,\n",
    "                 padding = 0,\n",
    "                 dilation = 1,\n",
    "                 num_layer = 3,\n",
    "                 dropout_rate = 0,\n",
    "                 batch_norm = False,\n",
    "        ):\n",
    "        # nn.Module 초기화\n",
    "        super(CNN2D, self).__init__()\n",
    "        self.embed = nn.Sequential(\n",
    "            nn.Conv2d(in_channel, out_channel, kernel_size, stride, padding, dilation),\n",
    "            nn.LeakyReLU(0.1)\n",
    "        )\n",
    "        # hidden layer 정의\n",
    "        self.hidden = nn.ModuleList()\n",
    "        for _ in range(num_layer):\n",
    "            hidden_layer = [] # List 형태로 만든 후 순차적으로 layer 요소 추가\n",
    "            hidden_layer.append(nn.Conv2d(out_channel, out_channel, kernel_size, stride, padding, dilation)) \n",
    "            hidden_layer.append(nn.LeakyReLU(0.1))\n",
    "\n",
    "            # Layer normalization, gradient가 발산/수렴하는 것을 방지해서 훈련 효율을 높임\n",
    "            # 둘 중 하나만 사용하면 됨\n",
    "            if dropout_rate != 0: # Dropout 정의\n",
    "                hidden_layer.append(nn.Dropout(dropout_rate))\n",
    "            if batch_norm: # batch normalization 정의\n",
    "                hidden_layer.append(nn.BatchNorm2d(out_channel))\n",
    "            hidden_layer.append(nn.MaxPool2d(2))\n",
    "            self.hidden.append(nn.Sequential(*hidden_layer))\n",
    "\n",
    "        self.output = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(512, num_output)\n",
    "        )\n",
    "    \n",
    "    # 순방향 함수 정의\n",
    "    # input x가 들어와서 어떤 연산을 거쳐 output이 될 지 정의하는 함수\n",
    "    def forward(self, x):\n",
    "        # embedding layer 통과\n",
    "        h = self.embed(x)\n",
    "#        print(h.shape)\n",
    "        # 각 layer를 순차적으로 통과\n",
    "        for layer in self.hidden:\n",
    "            h = layer(h)\n",
    "#            print(h.shape)\n",
    "        # output layer 통과\n",
    "        out = self.output(h)\n",
    "#        print(out.shape)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MNIST dataset 테스트"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1.4088],\n",
       "        [-0.2545],\n",
       "        [ 0.2423],\n",
       "        [ 0.7529],\n",
       "        [ 0.0350],\n",
       "        [ 0.6736],\n",
       "        [ 1.2637],\n",
       "        [ 1.0439],\n",
       "        [ 1.6194],\n",
       "        [ 1.4045],\n",
       "        [ 2.0430],\n",
       "        [ 0.6491],\n",
       "        [ 1.3756],\n",
       "        [ 0.7202],\n",
       "        [ 1.4177],\n",
       "        [ 0.6821],\n",
       "        [ 0.9159],\n",
       "        [ 1.4333],\n",
       "        [ 0.1086],\n",
       "        [ 0.4882],\n",
       "        [ 1.5869],\n",
       "        [-0.1379],\n",
       "        [ 0.7817],\n",
       "        [-0.3783],\n",
       "        [ 0.4409],\n",
       "        [ 1.8630],\n",
       "        [ 1.2650],\n",
       "        [ 1.4413],\n",
       "        [ 0.6946],\n",
       "        [ 1.2947],\n",
       "        [ 0.7333],\n",
       "        [-0.5676],\n",
       "        [ 1.0590],\n",
       "        [ 1.2692],\n",
       "        [ 0.7956],\n",
       "        [ 0.4946],\n",
       "        [-0.1548],\n",
       "        [ 0.8761],\n",
       "        [ 1.4045],\n",
       "        [ 0.4932],\n",
       "        [ 2.6240],\n",
       "        [-0.8794],\n",
       "        [-0.3385],\n",
       "        [ 0.8274],\n",
       "        [ 0.1018],\n",
       "        [ 1.1534],\n",
       "        [ 2.4967],\n",
       "        [ 1.4909],\n",
       "        [ 0.5622],\n",
       "        [ 2.1603],\n",
       "        [ 0.6098],\n",
       "        [ 1.1920],\n",
       "        [ 0.8038],\n",
       "        [ 0.9361],\n",
       "        [ 1.0951],\n",
       "        [-0.5228],\n",
       "        [ 0.5490],\n",
       "        [ 1.4619],\n",
       "        [ 0.5722],\n",
       "        [ 1.4929],\n",
       "        [-0.2679],\n",
       "        [ 1.1012],\n",
       "        [ 1.4119],\n",
       "        [-0.0358],\n",
       "        [ 0.5755],\n",
       "        [ 1.8174],\n",
       "        [ 2.2236],\n",
       "        [-0.9383],\n",
       "        [-0.2392],\n",
       "        [ 0.4964],\n",
       "        [ 1.0293],\n",
       "        [ 0.8888],\n",
       "        [ 0.5072],\n",
       "        [ 1.0473],\n",
       "        [ 0.3086],\n",
       "        [ 0.9178],\n",
       "        [ 0.6452],\n",
       "        [ 1.1547],\n",
       "        [ 0.5381],\n",
       "        [ 0.8815],\n",
       "        [ 1.0544],\n",
       "        [ 0.9762],\n",
       "        [ 1.1671],\n",
       "        [ 1.2662],\n",
       "        [ 2.7254],\n",
       "        [ 0.9513],\n",
       "        [ 0.3423],\n",
       "        [ 0.4087],\n",
       "        [-0.0332],\n",
       "        [-0.4666],\n",
       "        [ 0.1036],\n",
       "        [ 0.1517],\n",
       "        [ 0.8012],\n",
       "        [ 0.6099],\n",
       "        [ 1.4984],\n",
       "        [ 0.3509],\n",
       "        [ 1.3252],\n",
       "        [ 0.4421],\n",
       "        [ 0.4216],\n",
       "        [-0.1409]], grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with gzip.open('')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gnn",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
